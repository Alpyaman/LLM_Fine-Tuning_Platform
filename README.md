# ğŸ”¥ Serverless LLM Fine-Tuning Platform

A modern, production-ready platform for fine-tuning large language models using Unsloth for 2-5x faster training.

## ğŸ—ï¸ Architecture

**The Decoupled Trainer System:**

- **Frontend**: Next.js dashboard for uploads and monitoring
- **Backend**: FastAPI for API management and auth
- **Queue**: Redis + Celery for job orchestration
- **Engine**: Unsloth (GPU workers) for efficient training
- **Infrastructure**: RunPod/Lambda Labs + AWS S3

## ğŸ“‹ Project Status

### âœ… Phase 1: Core Engine (COMPLETED)
- [x] Unsloth integration for fast fine-tuning
- [x] Data formatter (Alpaca & ChatML)
- [x] SFTTrainer with LoRA adapters
- [x] Local training pipeline
- [x] Model export (.safetensors)

### âœ… Phase 2: Async Backend (COMPLETED)
- [x] FastAPI REST API
- [x] Redis + Celery job queue
- [x] File upload handling
- [x] Job status tracking
- [x] Worker monitoring (Flower)
- [x] Local & S3 storage support

### ğŸ”„ Phase 3-5: Coming Soon
- [ ] Next.js Frontend Dashboard
- [ ] Authentication & User Management
- [ ] Cloud Deployment (RunPod/Lambda Labs)

## ğŸš€ Quick Start

### Prerequisites
- Python 3.10+
- CUDA-capable GPU (or use Google Colab for Phase 1)
- Docker Desktop (for Phase 2)
- Hugging Face account

### Phase 1: Local Training

```bash
# Install dependencies
pip install -r requirements.txt

# Setup environment
copy .env.example .env
# Edit .env and add your HF_TOKEN

# Train locally
python train.py --data example_data.jsonl --max-steps 60
```

See [PHASE1_GUIDE.md](PHASE1_GUIDE.md) for details.

### Phase 2: API Server

```bash
# Start services (Windows)
start.bat

# Or manually:
# 1. Start Redis
docker-compose up -d

# 2. Start Celery Worker
celery -A celery_config worker --loglevel=info -Q training -P solo

# 3. Start API Server
python api.py
```

**Test the API:**
```bash
python test_api.py
```

See Phase 1 - Core Engine
â”‚   â”œâ”€â”€ train.py              # Main training script
â”‚   â”œâ”€â”€ inference.py          # Model inference
â”‚   â”œâ”€â”€ data_formatter.py     # Data formatting
â”‚   â”œâ”€â”€ config.py             # Training configuration
â”‚   â”œâ”€â”€ PHASE1_GUIDE.md       # Phase 1 guide
â”‚   â””â”€â”€ Colab_Fine_Tuning.ipynb
â”‚
â”œâ”€â”€ Phase 2 - Async Backend
â”‚   â”œâ”€â”€ api.py                # FastAPI application
â”‚   â”œâ”€â”€ celery_config.py      # Celery configuration
â”‚   â”œâ”€â”€ celery_worker.py      # Background tasks
â”‚   â”œâ”€â”€ storage.py            # Storage utilities
â”‚   â”œâ”€â”€ docker-compose.yml    # Redis + Flower
â”‚   â”œâ”€â”€ PHASE2_GUIDE.md       # Phase 2 guide
â”‚   â”œâ”€â”€ test_api.py           # API test suite
â”‚   â””â”€â”€ start.bat / start.sh  # Startup scripts
â”‚
â”œâ”€â”€ requirements.txt      # All dependencies
â”œâ”€â”€ example_data.jsonl    # Sample dataset
â”œâ”€â”€ .env.example          # Environment template
â””â”€â”€ storage/              # Data storage (created)
    â”œâ”€â”€ datasets/         # Uploaded datasets
    â””â”€â”€ models/           # Trained modelation
â”œâ”€â”€ requirements.txt      # Python dependencies
â”œâ”€â”€ example_data.jsonl    # Sample dataset
â”œâ”€â”€ PHASE1_GUIDE.md       # Detailed Phase 1 guide
â”œâ”€â”€ .env.example          # Environment variables template
â””â”€â”€ outputs/              # Training outputs (created)
    â””â”€â”€ adapter/          # LoRA adapter files
```

## ğŸ“Š Data Format

Your training data should be in JSONL format:

```json
{"instruction": "What is AI?", "output": "AI is...", "input": ""}
{"instruction": "Translate to Spanish", "output": "Hola", "input": "Hello"}
```

## ğŸ¯ Key Features

- **âš¡ Unsl- Training Engine:**
- Unsloth - Fast LLM fine-tuning
- PyTorch - Deep learning framework
- Transformers - HuggingFace models
- TRL - Transformer Reinforcement Learning
- PEFT - Parameter-Efficient Fine-Tuning

**Phase 2 - Backend API:**
- FastAPI - REST API framework
- Redis - Message broker
- Celery - Distributed task queue
- Flower - Task monitoring UI
- Pydantic - Data validation
- Docker - Containerization

**Coming Soon:**ata_formatter.py) - How to prepare your data
- [Configuration](config.py) - Training parameters explained
Phase 1: Local Training

```bash
# Basic training
python train.py --data my_data.jsonl --max-steps 60

# Advanced training
python train.py \
    --data dataset.jsonl \
    --model unsloth/llama-3-8b-bnb-4bit \
    --max-steps 200 \
    --batch-size 4 \
    --learning-rate 3e-4 \
    --lora-r 32 \
    --save-merged

# Test model
python inference.py --adapter adapter.zip
```

### Phase 2: API Usage

```bash
# Upload dataset
curl -X POST "http://localhost:8000/upload" \
  -F "file=@my_data.jsonl"

# Start training
curl -X POST "http://localhost:8000/train" \
  -H "Content-Type: application/json" \
  -d '{
    "dataset_filename": "my_data.jsonl",
    "config": {
      "max_steps": 100,
      "batch_size": 2,
      "learning_rate": 0.0002
    }
  }'

# Check status
curl "http://localhost:8000/status/{job_id}"
### Advanced Training
```bash
python train.py \
    --data dataset.jsonl \
    --model unsloth/llama-3-8b-bnb-4bit \
    --format alpaca \
    --max-steps 200 \
    --batch-size 4 \
    --learning-rate 3e-4 \
    --lora-r 32 \
    --save-merged \
    --quantize q4_k_m
```

### Format Data
```bash
python data_formatter.py \
    --input raw_data.jsonl \
    --output formatted_data.jsonl \
    --format alpaca
```

## ğŸ› Troubleshooting

**Out of Memory?**
- Reduce `--batch-size` to 1
- Lower `max_seq_length` in config.py

**Model Not Downloading?**
- Check `HF_TOKEN` in `.env`
- Accept model license on HuggingFace

**Import Errors?**
- Activate virtual environment
- Run `pip install --upgrade unsloth`

## ğŸ¤ Contributing

This is a learning project following a 5-phase roadmap. Contributions welcome after Phase 5 completion!

## ğŸ“ License

MIT License - see [LICENSE](LICENSE) file

## ğŸ™ Credits

- **Unsloth** - For blazing fast fine-tuning
- **HuggingFace** - For transformers and model hub
- **RunPod/Lambda Labs** - GPU infrastructure partners

---

**Status**: ğŸŸ¢ Phase 2 Complete | Ready for Phase 3

Built with â¤ï¸ for the AI community