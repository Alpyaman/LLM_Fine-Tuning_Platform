{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "349bf115",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ Setup Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef8611a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Unsloth and dependencies\n",
    "%%capture\n",
    "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
    "!pip install --no-deps trl peft accelerate bitsandbytes jsonlines python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "623aff1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify GPU availability\n",
    "import torch\n",
    "print(f\"üî• PyTorch version: {torch.__version__}\")\n",
    "print(f\"üéÆ CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"üéØ GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"üíæ GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No GPU found! Please enable GPU runtime.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4de1244",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46ad46b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional\n",
    "\n",
    "@dataclass\n",
    "class TrainingConfig:\n",
    "    \"\"\"Configuration for model fine-tuning\"\"\"\n",
    "    \n",
    "    # Model\n",
    "    base_model: str = \"unsloth/llama-3-8b-bnb-4bit\"\n",
    "    max_seq_length: int = 2048\n",
    "    load_in_4bit: bool = True\n",
    "    \n",
    "    # LoRA Configuration\n",
    "    lora_r: int = 16\n",
    "    lora_alpha: int = 16\n",
    "    lora_dropout: float = 0.0\n",
    "    target_modules: list = None\n",
    "    \n",
    "    # Training Hyperparameters\n",
    "    batch_size: int = 2\n",
    "    gradient_accumulation_steps: int = 4\n",
    "    warmup_steps: int = 5\n",
    "    max_steps: int = 60\n",
    "    learning_rate: float = 2e-4\n",
    "    fp16: bool = False\n",
    "    bf16: bool = True\n",
    "    \n",
    "    # Optimizer\n",
    "    optim: str = \"adamw_8bit\"\n",
    "    weight_decay: float = 0.01\n",
    "    lr_scheduler_type: str = \"linear\"\n",
    "    \n",
    "    # Logging\n",
    "    logging_steps: int = 1\n",
    "    \n",
    "    # Output\n",
    "    output_dir: str = \"./outputs\"\n",
    "    logging_dir: str = \"./logs\"\n",
    "    save_steps: int = 25\n",
    "    \n",
    "    # Data\n",
    "    dataset_text_field: str = \"text\"\n",
    "    packing: bool = False\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        if self.target_modules is None:\n",
    "            self.target_modules = [\n",
    "                \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                \"gate_proj\", \"up_proj\", \"down_proj\"\n",
    "            ]\n",
    "\n",
    "# Initialize configuration\n",
    "config = TrainingConfig()\n",
    "print(\"‚úÖ Configuration loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca738cf3",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ Data Formatter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c223152c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jsonlines\n",
    "from typing import List, Dict, Optional\n",
    "from pathlib import Path\n",
    "\n",
    "class DataFormatter:\n",
    "    \"\"\"Handles conversion of various data formats to instruction format\"\"\"\n",
    "    \n",
    "    ALPACA_PROMPT = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{instruction}\n",
    "\n",
    "### Input:\n",
    "{input}\n",
    "\n",
    "### Response:\n",
    "{output}\"\"\"\n",
    "\n",
    "    ALPACA_PROMPT_NO_INPUT = \"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{instruction}\n",
    "\n",
    "### Response:\n",
    "{output}\"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def format_alpaca(\n",
    "        instruction: str,\n",
    "        output: str,\n",
    "        input_text: Optional[str] = None\n",
    "    ) -> Dict[str, str]:\n",
    "        if input_text and input_text.strip():\n",
    "            text = DataFormatter.ALPACA_PROMPT.format(\n",
    "                instruction=instruction,\n",
    "                input=input_text,\n",
    "                output=output\n",
    "            )\n",
    "        else:\n",
    "            text = DataFormatter.ALPACA_PROMPT_NO_INPUT.format(\n",
    "                instruction=instruction,\n",
    "                output=output\n",
    "            )\n",
    "        return {\"text\": text}\n",
    "    \n",
    "    @staticmethod\n",
    "    def load_and_format_jsonl(\n",
    "        input_path: str,\n",
    "        format_type: str = \"alpaca\"\n",
    "    ) -> List[Dict[str, str]]:\n",
    "        formatted_data = []\n",
    "        with jsonlines.open(input_path) as reader:\n",
    "            for obj in reader:\n",
    "                instruction = obj.get(\"instruction\", \"\")\n",
    "                output = obj.get(\"output\", \"\")\n",
    "                input_text = obj.get(\"input\", \"\")\n",
    "                \n",
    "                if not instruction or not output:\n",
    "                    continue\n",
    "                \n",
    "                formatted = DataFormatter.format_alpaca(\n",
    "                    instruction, output, input_text\n",
    "                )\n",
    "                formatted_data.append(formatted)\n",
    "        return formatted_data\n",
    "\n",
    "print(\"‚úÖ DataFormatter loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e76a2139",
   "metadata": {},
   "source": [
    "## 4Ô∏è‚É£ Create Sample Dataset (or Upload Your Own)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e9fa266",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create example dataset\n",
    "import json\n",
    "\n",
    "example_data = [\n",
    "    {\"instruction\": \"What is the capital of France?\", \"output\": \"The capital of France is Paris. It is located in the north-central part of the country and is known for its art, culture, and historical landmarks like the Eiffel Tower.\", \"input\": \"\"},\n",
    "    {\"instruction\": \"Explain what machine learning is\", \"output\": \"Machine learning is a subset of artificial intelligence that enables computers to learn and improve from experience without being explicitly programmed. It uses algorithms and statistical models to analyze and draw inferences from patterns in data.\", \"input\": \"\"},\n",
    "    {\"instruction\": \"Write a Python function to calculate factorial\", \"output\": \"Here's a Python function to calculate factorial:\\n\\ndef factorial(n):\\n    if n == 0 or n == 1:\\n        return 1\\n    else:\\n        return n * factorial(n - 1)\\n\\n# Example usage:\\nprint(factorial(5))  # Output: 120\", \"input\": \"\"},\n",
    "    {\"instruction\": \"Translate the following text to Spanish\", \"output\": \"¬°Hola! ¬øC√≥mo est√°s? Espero que est√©s teniendo un buen d√≠a.\", \"input\": \"Hello! How are you? I hope you're having a good day.\"},\n",
    "    {\"instruction\": \"What are the benefits of regular exercise?\", \"output\": \"Regular exercise offers numerous benefits including: improved cardiovascular health, stronger muscles and bones, better mental health and mood, enhanced immune system, weight management, increased energy levels, better sleep quality, and reduced risk of chronic diseases like diabetes and heart disease.\", \"input\": \"\"}\n",
    "]\n",
    "\n",
    "# Save to file\n",
    "with jsonlines.open('example_data.jsonl', mode='w') as writer:\n",
    "    writer.write_all(example_data)\n",
    "\n",
    "print(f\"‚úÖ Created example dataset with {len(example_data)} examples\")\n",
    "print(\"\\nüí° To use your own data:\")\n",
    "print(\"   1. Upload your JSONL file using the file browser (left sidebar)\")\n",
    "print(\"   2. Change DATA_PATH below to your filename\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d464d419",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set your data path here\n",
    "DATA_PATH = \"example_data.jsonl\"  # Change this to your uploaded file\n",
    "\n",
    "# Optional: Upload your own data\n",
    "# from google.colab import files\n",
    "# uploaded = files.upload()\n",
    "# DATA_PATH = list(uploaded.keys())[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59ba3f53",
   "metadata": {},
   "source": [
    "## 5Ô∏è‚É£ Load Model & Setup LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c25174d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "\n",
    "print(f\"üì¶ Loading model: {config.base_model}\")\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=config.base_model,\n",
    "    max_seq_length=config.max_seq_length,\n",
    "    dtype=None,  # Auto-detect\n",
    "    load_in_4bit=config.load_in_4bit,\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Model loaded successfully\")\n",
    "\n",
    "# Setup LoRA\n",
    "print(\"üîß Setting up LoRA adapters...\")\n",
    "\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=config.lora_r,\n",
    "    target_modules=config.target_modules,\n",
    "    lora_alpha=config.lora_alpha,\n",
    "    lora_dropout=config.lora_dropout,\n",
    "    bias=\"none\",\n",
    "    use_gradient_checkpointing=\"unsloth\",\n",
    "    random_state=3407,\n",
    "    use_rslora=False,\n",
    "    loftq_config=None,\n",
    ")\n",
    "\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"‚úÖ LoRA configured: {trainable_params:,} / {total_params:,} parameters trainable \"\n",
    "      f\"({100 * trainable_params / total_params:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c294f94",
   "metadata": {},
   "source": [
    "## 6Ô∏è‚É£ Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a137ea38",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "print(f\"üìÅ Loading dataset from: {DATA_PATH}\")\n",
    "\n",
    "# Load dataset\n",
    "dataset = load_dataset(\"json\", data_files=DATA_PATH, split=\"train\")\n",
    "\n",
    "# Format if needed\n",
    "if \"text\" not in dataset.column_names:\n",
    "    print(\"‚ö†Ô∏è Formatting data...\")\n",
    "    formatted_data = DataFormatter.load_and_format_jsonl(DATA_PATH)\n",
    "    \n",
    "    # Save and reload\n",
    "    with jsonlines.open('formatted_temp.jsonl', mode='w') as writer:\n",
    "        writer.write_all(formatted_data)\n",
    "    dataset = load_dataset(\"json\", data_files='formatted_temp.jsonl', split=\"train\")\n",
    "\n",
    "print(f\"‚úÖ Dataset loaded: {len(dataset)} examples\")\n",
    "print(f\"\\nüìã Preview first example:\")\n",
    "print(dataset[0]['text'][:500] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05d7d12a",
   "metadata": {},
   "source": [
    "## 7Ô∏è‚É£ Train the Model üöÄ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70d2b866",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "from trl import SFTTrainer\n",
    "import os\n",
    "\n",
    "print(\"üöÄ Starting training...\")\n",
    "print(f\"   Max steps: {config.max_steps}\")\n",
    "print(f\"   Batch size: {config.batch_size}\")\n",
    "print(f\"   Learning rate: {config.learning_rate}\")\n",
    "\n",
    "# Create output directory\n",
    "os.makedirs(config.output_dir, exist_ok=True)\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=config.output_dir,\n",
    "    per_device_train_batch_size=config.batch_size,\n",
    "    gradient_accumulation_steps=config.gradient_accumulation_steps,\n",
    "    warmup_steps=config.warmup_steps,\n",
    "    max_steps=config.max_steps,\n",
    "    learning_rate=config.learning_rate,\n",
    "    fp16=not torch.cuda.is_bf16_supported(),\n",
    "    bf16=torch.cuda.is_bf16_supported(),\n",
    "    logging_steps=config.logging_steps,\n",
    "    optim=config.optim,\n",
    "    weight_decay=config.weight_decay,\n",
    "    lr_scheduler_type=config.lr_scheduler_type,\n",
    "    seed=3407,\n",
    "    save_steps=config.save_steps,\n",
    "    save_total_limit=2,\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "# Initialize trainer\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=dataset,\n",
    "    dataset_text_field=config.dataset_text_field,\n",
    "    max_seq_length=config.max_seq_length,\n",
    "    dataset_num_proc=2,\n",
    "    packing=config.packing,\n",
    "    args=training_args,\n",
    ")\n",
    "\n",
    "# Start training\n",
    "trainer.train()\n",
    "\n",
    "print(\"\\n‚úÖ Training completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f72cb3b3",
   "metadata": {},
   "source": [
    "## 8Ô∏è‚É£ Save the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "851eeafe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save LoRA adapter\n",
    "adapter_dir = \"./outputs/adapter\"\n",
    "os.makedirs(adapter_dir, exist_ok=True)\n",
    "\n",
    "print(f\"üíæ Saving adapter to: {adapter_dir}\")\n",
    "model.save_pretrained(adapter_dir)\n",
    "tokenizer.save_pretrained(adapter_dir)\n",
    "\n",
    "print(\"‚úÖ Adapter saved successfully!\")\n",
    "print(f\"   Files: adapter_config.json, adapter_model.safetensors\")\n",
    "\n",
    "# List saved files\n",
    "print(\"\\nüìÅ Saved files:\")\n",
    "!ls -lh {adapter_dir}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "360185fd",
   "metadata": {},
   "source": [
    "## 9Ô∏è‚É£ Test the Model (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d208883e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable inference mode\n",
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "# Test prompt\n",
    "test_instruction = \"What is artificial intelligence?\"\n",
    "alpaca_prompt = \"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{}\n",
    "\n",
    "### Response:\n",
    "{}\"\"\"\n",
    "\n",
    "inputs = tokenizer(\n",
    "    [alpaca_prompt.format(test_instruction, \"\")],\n",
    "    return_tensors=\"pt\"\n",
    ").to(\"cuda\")\n",
    "\n",
    "print(\"ü§ñ Generating response...\\n\")\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=128,\n",
    "    temperature=0.7,\n",
    "    top_p=0.9,\n",
    "    do_sample=True\n",
    ")\n",
    "\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e427c7",
   "metadata": {},
   "source": [
    "## üîü Download Your Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bf2b7c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zip the adapter folder for download\n",
    "!zip -r adapter.zip ./outputs/adapter/\n",
    "\n",
    "print(\"‚úÖ Model zipped!\")\n",
    "print(\"\\nüì• To download:\")\n",
    "print(\"   1. Find 'adapter.zip' in the file browser\")\n",
    "print(\"   2. Right-click ‚Üí Download\")\n",
    "print(\"\\nOr use the code below:\")\n",
    "\n",
    "from google.colab import files\n",
    "files.download('adapter.zip')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f79e7ba",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéâ Done!\n",
    "\n",
    "You now have a fine-tuned LoRA adapter! \n",
    "\n",
    "### Next Steps:\n",
    "1. Download your adapter\n",
    "2. Load it locally with:\n",
    "```python\n",
    "from unsloth import FastLanguageModel\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=\"path/to/adapter\",\n",
    "    max_seq_length=2048,\n",
    ")\n",
    "```\n",
    "\n",
    "### Tips:\n",
    "- **Longer training**: Increase `config.max_steps` to 200-500\n",
    "- **Better results**: Use more training data (1000+ examples)\n",
    "- **Different model**: Change `config.base_model` to other Unsloth models\n",
    "- **Save merged model**: Uncomment section 8 alternative\n",
    "\n",
    "### Resources:\n",
    "- [Unsloth Documentation](https://github.com/unslothai/unsloth)\n",
    "- [HuggingFace Models](https://huggingface.co/models)\n",
    "- [Project GitHub](your-repo-url)\n",
    "\n",
    "---\n",
    "\n",
    "**Made with ‚ù§Ô∏è for the AI community**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
